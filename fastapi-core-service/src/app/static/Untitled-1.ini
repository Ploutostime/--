import os
import json
from typing import List, Dict, Optional, Tuple
from langchain.llms import OpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# Persona 管理（简单 JSON 存储）
class PersonaManager:
    def __init__(self, path: str = "personas.json"):
        self.path = path
        self.personas = self._load()

    def _load(self) -> Dict[str, Dict]:
        if os.path.exists(self.path):
            with open(self.path, "r", encoding="utf-8") as f:
                return json.load(f)
        # 默认示例 persona
        default = {
            "agri_expert": {
                "name": "农业专家",
                "instruction": "你是一个专业的农业顾问，回答应简洁、有行动建议、引用可靠来源。"
            },
            "friendly_bot": {
                "name": "亲切助手",
                "instruction": "你是一个语气亲切的助手，回答通俗易懂、鼓励用户。"
            }
        }
        with open(self.path, "w", encoding="utf-8") as f:
            json.dump(default, f, ensure_ascii=False, indent=2)
        return default

    def get(self, persona_id: str) -> Dict:
        return self.personas.get(persona_id, {"name": "default", "instruction": ""})

    def add(self, persona_id: str, name: str, instruction: str):
        self.personas[persona_id] = {"name": name, "instruction": instruction}
        with open(self.path, "w", encoding="utf-8") as f:
            json.dump(self.personas, f, ensure_ascii=False, indent=2)


# RAG 存储与检索（FAISS + OpenAI embeddings）
class RAGStore:
    def __init__(self, persist_path: str = "rag_faiss", embedding_model=None):
        self.persist_path = persist_path
        self.embedding_model = embedding_model or OpenAIEmbeddings()
        self.vstore: Optional[FAISS] = None
        self._load_or_init()

    def _load_or_init(self):
        if os.path.exists(self.persist_path):
            try:
                self.vstore = FAISS.load_local(self.persist_path, self.embedding_model)
            except Exception:
                self.vstore = None

    def add_documents(self, docs: List[str], metadatas: Optional[List[dict]] = None):
        # 创建或追加文档
        from langchain.docstore.document import Document
        documents = [Document(page_content=d, metadata=(m or {})) for d, m in zip(docs, metadatas or [{}]*len(docs))]
        if self.vstore is None:
            self.vstore = FAISS.from_documents(documents, self.embedding_model)
        else:
            self.vstore.add_documents(documents)
        self.vstore.save_local(self.persist_path)

    def retrieve(self, query: str, k: int = 4):
        if self.vstore is None:
            return []
        docs_and_scores = self.vstore.similarity_search_with_score(query, k=k)
        # 返回文本与分数
        return [{"text": d.page_content, "score": s, "metadata": d.metadata} for d, s in docs_and_scores]


# Orchestrator：组合 persona + RAG + LLM
class AgentOrchestrator:
    def __init__(self, llm=None, rag_store: Optional[RAGStore] = None, persona_manager: Optional[PersonaManager] = None):
        # LLM 可以替换为其他 langchain-compatible LLM
        self.llm = llm or OpenAI(temperature=0.2)
        self.rag = rag_store or RAGStore()
        self.personas = persona_manager or PersonaManager()
        # PromptTemplate 用于注入 persona 指令与检索上下文
        self.template = PromptTemplate(
            input_variables=["persona_instruction", "context", "user_input"],
            template=(
                "Persona 指令：{persona_instruction}\n\n"
                "检索到的参考资料（请只使用相关段落并在回答末尾列出来源）:\n{context}\n\n"
                "用户问题：{user_input}\n\n"
                "请给出简洁的回答，列出必要的步骤或建议，若不确定请说明并提供可验证的参考。"
            ),
        )

    def _build_context_str(self, retrieved: List[dict]) -> str:
        if not retrieved:
            return "（未检索到相关资料）"
        parts = []
        for i, r in enumerate(retrieved, 1):
            md = r.get("metadata") or {}
            src = md.get("source") or f"doc#{i}"
            parts.append(f"[{src}] (score:{r.get('score'):.3f}):\n{r.get('text')}")
        return "\n\n".join(parts)

    def run(self, user_input: str, persona_id: str = "agri_expert", k: int = 4) -> Tuple[str, List[dict]]:
        persona = self.personas.get(persona_id)
        retrieved = self.rag.retrieve(user_input, k=k)
        context_str = self._build_context_str(retrieved)
        prompt = self.template.format(
            persona_instruction=persona.get("instruction", ""),
            context=context_str,
            user_input=user_input
        )
        # 使用 RetrievalQA 或简单 LLM 调用均可；这里直接传入拼好的 prompt。
        answer = self.llm(prompt)
        return answer, retrieved